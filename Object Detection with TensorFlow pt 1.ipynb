{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-statement\" data-toc-modified-id=\"Problem-statement-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Problem statement</a></span></li><li><span><a href=\"#Motivation-and-Applications\" data-toc-modified-id=\"Motivation-and-Applications-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Motivation and Applications</a></span></li><li><span><a href=\"#Why-Transfer-Learning\" data-toc-modified-id=\"Why-Transfer-Learning-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Why Transfer Learning</a></span></li></ul></li><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Background</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-a-Neural-Network\" data-toc-modified-id=\"What-is-a-Neural-Network-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>What is a Neural Network</a></span></li><li><span><a href=\"#What-is-Transfer-Learning\" data-toc-modified-id=\"What-is-Transfer-Learning-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>What is Transfer Learning</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Installing-TensorFlow-and-Object-Detection-API\" data-toc-modified-id=\"Installing-TensorFlow-and-Object-Detection-API-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Installing TensorFlow and Object Detection API</a></span></li><li><span><a href=\"#Preparing-the-Data\" data-toc-modified-id=\"Preparing-the-Data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Preparing the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-Images-from-Video\" data-toc-modified-id=\"Extract-Images-from-Video-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Extract Images from Video</a></span></li><li><span><a href=\"#Create-CSV-files-for-training/testing\" data-toc-modified-id=\"Create-CSV-files-for-training/testing-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Create CSV files for training/testing</a></span></li><li><span><a href=\"#Convert-CSV-to-TFR-Format\" data-toc-modified-id=\"Convert-CSV-to-TFR-Format-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Convert CSV to TFR Format</a></span></li></ul></li><li><span><a href=\"#Training-the-Model\" data-toc-modified-id=\"Training-the-Model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Training the Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preparing-the-environment\" data-toc-modified-id=\"Preparing-the-environment-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Preparing the environment</a></span></li></ul></li></ul></li><li><span><a href=\"#part-2\" data-toc-modified-id=\"part-2-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>part 2</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>My goal in this series is to deploy a neural network capable of identifying and localizing pedestrians in an image (the combination of image classification and localization is called object detection). In the first part of the series I will do this by downloading a pretrained model and using transfer learning to fine tune it for my problem. In the second part, I will try to create and deploy a model from scratch.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation and Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>My main motivation for this project was simply to gain an understanding of deep learning, a field I knew nothing about prior to this project. Furthermore, object detection problems tend to involve much deeper networks than object classification, and the concepts involved here are highly transferable across other deep learning domains. </p>\n",
    "<p>Of course, object detection has some pretty cool applications in its own right. For example, security companies build on top of object detection models to do things like gait analysis and tracking people across multiple cameras. Self-driving cars need to be able to perform object detection to avoid hitting pedestrians and other cars. And one could imagine lots of fun home applications for object detection.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practically speaking, you will almost always use transfer learning when dealing with neural network. The reason for this is twofold:\n",
    "1. Using a pretrained model drastically cuts down the amount of resources needed to fine tune a network. By using a pretrained model you will require fewer training samples and less computing time for a comparable level of accuracy.\n",
    "2. It is unlikely that you will be creating a deep learning model that is completely new. Most new models are really variations of existing models, so it makes sense to take advantage of existing work\n",
    "Consequently, transfer learning is one of the most important skills you can have with respect to deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively speaking, we can think of transfer learning in the following way: When a deep learning model is trained for an application it develops insights. For example, a deep learning network trained for image recognition may learn how to recognize different shapes and textures and use those attributes as features. So if we have are performing a similar task on new data we want to keep those ins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is essentially repurposing an an existing deep learning model for a new but similar task. \n",
    "\n",
    "To understand when and why transfer learning is effective, remember that the layers in a trained model represent features that the model has learned to extract. Furthermore, the features extracted tend to become more particular to the problem as we go into deeper layers. For example, an animal classifier may have a layer that extracts basic shapes from the image, another layer that recognizes textures, and so on. As you can see from the example, the features a model learns to extract are often useful for similar tasks. So if we wanted to create a face detector, we could start from randomized layers and see which features emerged from our model. Or we could take advantage of the fact that the features extracted by our first model are useful for other image classification tasks, and use that as a starting point rather than reinvent the wheel.\n",
    "\n",
    "\n",
    "We can break down\n",
    "transfer learning into the following steps:\n",
    "1. download a model that has already been trained on a dataset\n",
    "2. make whatever adjustments you want to test on the model (e.g. adjust the learning rate)\n",
    "3. strip last layers of the existing model and replace them with randomized layers\n",
    "4. fine-tune layers on new data\n",
    "\n",
    "A slightly more technical way of looking at transfer learning is from an optimization perspective. Remember, a neural network is composed of layers of weights that transform the data in each layer. And when we train a model, we are moving those weights to a more optimal value through gradient descent. When we use a new model, those weights' values are completely random to start. However, if we assume similar input and output, then the weights of our pretrained model are likely closer to their optimum values than completely random weights. So our pretrained model requires less adjustment (training steps) to be optimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing TensorFlow and Object Detection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I currently have 3 videos of pedestrians using crosswalks in different situations as well as CSV files for each video which give the bounding box information for the pedestrians in the video in YOLO format (x, y, height, width).</p>\n",
    "<p>My goal in this section is to break down each video into its component frames in jpg format. I then need to create a data frame which contains the following columns: file path, width, height, class, xmin, xmax, ymin, ymax</p>\n",
    "<p>It is also worth noting that all of my videos have the same format, so I don't need to do any image resizing. However, if I wanted to generalize a pipeline, I could perform a combination zero-padding and scaling to make sure all images were the same size.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Images from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of video files\n",
    "dataPath = 'data/'\n",
    "dataFiles = os.listdir(dataPath)\n",
    "videoFiles = [dataPath+file for file in dataFiles if file.endswith('.avi')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to turn video into images\n",
    "def frame_capture(path): \n",
    "    \n",
    "    cap = cv2.VideoCapture(path) \n",
    "    currentFrame = 0\n",
    "    directory = path.strip('.avi')\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory of data')\n",
    "\n",
    "    \n",
    "    #while(True):\n",
    "    while (currentFrame < 1300):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Saves image of the current frame in jpg file\n",
    "        name = directory + '/frame' + str(currentFrame) + '.jpg'\n",
    "        cv2.imwrite(name, frame)\n",
    "\n",
    "        # To stop duplicate images\n",
    "        currentFrame += 1\n",
    "\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn each video file into a directory of image files\n",
    "for file in videoFiles:\n",
    "    frame_capture(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV files for training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundingBoxFiles = ['data/night.csv', 'data/fourway.csv', 'data/crosswalk.csv']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reformat CSV data into required format\n",
    "pedestrian_labels = pd.DataFrame()\n",
    "for file in boundingBoxFiles:\n",
    "    name = file.replace('/','.').split('.')[1]\n",
    "    df = pd.read_csv(file)\n",
    "    new_df = pd.DataFrame()\n",
    "    #create columns for new dataframe\n",
    "    new_df['filename'] = df.index.astype(str)\n",
    "    new_df['filename'] = 'data/'+ name+ '/frame'+ new_df['filename']+ '.jpg'\n",
    "    new_df['width'] = df.w\n",
    "    new_df['height'] = df.h\n",
    "    new_df['class'] = 'pedestrian'\n",
    "    new_df['xmin'] = df.x\n",
    "    new_df['ymin'] = df.y-df.h\n",
    "    new_df['xmax'] = df.x + df.w\n",
    "    new_df['ymax'] = df.y\n",
    "    #store to central data frame\n",
    "    pedestrian_labels = pedestrian_labels.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and testing set, then save file\n",
    "train_labels = pedestrian_labels.sample(frac=0.8,random_state=17)\n",
    "test_labels = pedestrian_labels.drop(train_labels.index)\n",
    "pedestrian_labels.to_csv('data/pedestrian_labels.csv', index=False)\n",
    "train_labels.to_csv('data/train_labels.csv', index=False)\n",
    "test_labels.to_csv('data/test_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>data/night/frame118.jpg</td>\n",
       "      <td>156</td>\n",
       "      <td>312</td>\n",
       "      <td>pedestrian</td>\n",
       "      <td>1631</td>\n",
       "      <td>161</td>\n",
       "      <td>1787</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>data/night/frame400.jpg</td>\n",
       "      <td>172</td>\n",
       "      <td>345</td>\n",
       "      <td>pedestrian</td>\n",
       "      <td>1199</td>\n",
       "      <td>139</td>\n",
       "      <td>1371</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>data/night/frame439.jpg</td>\n",
       "      <td>145</td>\n",
       "      <td>291</td>\n",
       "      <td>pedestrian</td>\n",
       "      <td>1040</td>\n",
       "      <td>179</td>\n",
       "      <td>1185</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>data/night/frame452.jpg</td>\n",
       "      <td>143</td>\n",
       "      <td>286</td>\n",
       "      <td>pedestrian</td>\n",
       "      <td>966</td>\n",
       "      <td>183</td>\n",
       "      <td>1109</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>data/night/frame455.jpg</td>\n",
       "      <td>141</td>\n",
       "      <td>283</td>\n",
       "      <td>pedestrian</td>\n",
       "      <td>946</td>\n",
       "      <td>183</td>\n",
       "      <td>1087</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filename  width  height       class  xmin  ymin  xmax  \\\n",
       "118  data/night/frame118.jpg    156     312  pedestrian  1631   161  1787   \n",
       "400  data/night/frame400.jpg    172     345  pedestrian  1199   139  1371   \n",
       "439  data/night/frame439.jpg    145     291  pedestrian  1040   179  1185   \n",
       "452  data/night/frame452.jpg    143     286  pedestrian   966   183  1109   \n",
       "455  data/night/frame455.jpg    141     283  pedestrian   946   183  1087   \n",
       "\n",
       "     ymax  \n",
       "118   473  \n",
       "400   484  \n",
       "439   470  \n",
       "452   469  \n",
       "455   466  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert CSV to TFR Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert my files to TensorFlowRecords format, I borrowed this <a href=\"https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py\">script</a> which comes from the guide I used to learn the object detection API, see this <a href=https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9>link</a> for reference. I simply had to change the file paths to correspond to where I was storing my data, and change \"racoon\" to \"pedestrian.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> If you have not installed the object detection API already, you can follow this <a href=\"https://medium.com/@viviennediegoencarnacion/how-to-setup-tensorflow-object-detection-on-mac-a0b72fbf470a\">guide</a>. Once you have it installed, you can choose a pre-trained model from <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\">here</a>. I chose this <a href=\"http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz\">model</a> and <a href=https://github.com/tensorflow/models/blob/a4944a57ad2811e1f6a7a87589a9fc8a776e8d3c/object_detection/samples/configs/ssd_mobilenet_v1_pets.config>checkpoint</a> to get started.</p>\n",
    "<p>Once I had everything downloaded, I created a folder named \"training\", and created a file named \"label_map.pbtxt.\" I then placed moved the config file from the tar folder containing my pre-trained model into \"training\" adjusted the number of classes to 1, reduced the batch size to accommodate for my machine's memory, and changed the filepaths for the testing data, training data, and the label map. Finally, I made a copy \"train.py\" from \"TensorFlow/models/research/object_detection/legacy/train.py\" inside my working directory</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['data/train.record']\n",
    "raw_image_dataset = tf.data.TFRecordDataset(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: {depth: (), height: (), image_raw: (), label: (), width: ()}, types: {depth: tf.int64, height: tf.int64, image_raw: tf.string, label: tf.int64, width: tf.int64}>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary describing the features.  \n",
    "image_feature_description = {\n",
    "    'height': tf.FixedLenFeature([], tf.int64),\n",
    "    'width': tf.FixedLenFeature([], tf.int64),\n",
    "    'depth': tf.FixedLenFeature([], tf.int64),\n",
    "    'label': tf.FixedLenFeature([], tf.int64),\n",
    "    'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Feature: depth (data type: int64) is required but could not be found.\n\t [[Node: ParseSingleExample/ParseSingleExample = ParseSingleExample[Tdense=[DT_INT64, DT_INT64, DT_STRING, DT_INT64, DT_INT64], dense_keys=[\"depth\", \"height\", \"image_raw\", \"label\", \"width\"], dense_shapes=[[], [], [], [], []], num_sparse=0, sparse_keys=[], sparse_types=[]](arg0, ParseSingleExample/Const, ParseSingleExample/Const_1, ParseSingleExample/Const_2, ParseSingleExample/Const_3, ParseSingleExample/Const_4)]] [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a88755b4cdb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage_features\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed_image_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mimage_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \"\"\"\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m       return sparse.deserialize_sparse_tensors(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1857\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Feature: depth (data type: int64) is required but could not be found.\n\t [[Node: ParseSingleExample/ParseSingleExample = ParseSingleExample[Tdense=[DT_INT64, DT_INT64, DT_STRING, DT_INT64, DT_INT64], dense_keys=[\"depth\", \"height\", \"image_raw\", \"label\", \"width\"], dense_shapes=[[], [], [], [], []], num_sparse=0, sparse_keys=[], sparse_types=[]](arg0, ParseSingleExample/Const, ParseSingleExample/Const_1, ParseSingleExample/Const_2, ParseSingleExample/Const_3, ParseSingleExample/Const_4)]] [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "for image_features in parsed_image_dataset.take(1):\n",
    "  image_raw = image_features['image_raw'].numpy()\n",
    "  display.display(display.Image(data=image_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
